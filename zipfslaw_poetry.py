# -*- coding: utf-8 -*-
"""ZipfsLaw_Poetry.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t6fiGwwqPUbA3mTv2W7ApsNKNHAqozI1
"""

# Zipfâ€™s Law of Abbreviation applied to poetry
# Author: Eva Florensa Villacampa

## OPENING THE URLs AND GETTING THE TEXT

import urllib.request # Module for opening the URLs containing the books of poems

# Dictionary with the URLs of the four books of poems: two in Catalan and two in English.
book_urls = {
    'eng1': 'https://www.gutenberg.org/cache/epub/48371/pg48371.txt',
    'eng2': 'https://www.gutenberg.org/cache/epub/60337/pg60337.txt',
    'cat1': 'https://www.gutenberg.org/cache/epub/52409/pg52409.txt',
    'cat2': 'https://www.gutenberg.org/cache/epub/56866/pg56866.txt',
}

# Dictionary to keep the text of the books
book_texts = {}

# Loop to iterate through the dictionary: for each URL, get the text.
for key, url in book_urls.items():
  book_text = urllib.request.urlopen(url) # Open URL
  book_text = book_text.read() # Read the content
  book_text = book_text.decode("utf-8") # Decode as UTF-8

  # Keep the text in the dictionary
  book_texts[key] = book_text

# Access dictionary and save the text of each book in a different variable.
eng1_text = book_texts.get('eng1', '')
eng2_text = book_texts.get('eng2', '')
cat1_text = book_texts.get('cat1', '')
cat2_text = book_texts.get('cat2', '')

## PRE-PROCESSING

import re # Package to use regular expressions
# Cleaning book eng1 by removing the information of the introduction and the end that is not part of the book
eng1_cleaned = re.sub("^.*?British Library", "", eng1_text, flags=re.DOTALL) # Substituting everything before British library with nothing, to cut what there was before. Use re.DOTALL to make the pattern match with multiple lines.
eng1_cleaned = re.sub("END OF.*$", "", eng1_cleaned, flags=re.DOTALL) # Susbtituting everythin after END OF with nothing, to cut it.

# Cleaning book eng2, using the same procedure as with eng1.
eng2_cleaned = re.sub("^.*?POEMS", "POEMS", eng2_text, flags=re.DOTALL)
eng2_cleaned = re.sub("End of Project.*$", "", eng2_cleaned, flags=re.DOTALL)

# Cleaning book cat1 by removing the English parts at the introduction and at the end, using the same procedure as with the English books.
cat1_cleaned = re.sub("^.*?JOAN", "JOAN", cat1_text, flags=re.DOTALL)
cat1_cleaned = re.sub("END OF.*$", "", cat1_cleaned, flags=re.DOTALL)

# Cleaning book cat2, using the same procedure as with cat1.
cat2_cleaned = re.sub("^.*?JOAN", "JOAN", cat2_text, flags=re.DOTALL)
cat2_cleaned = re.sub("END OF.*$", "", cat2_cleaned, flags=re.DOTALL)

# Tokenize all books, by splitting them at spaces
eng1_tokenized = eng1_cleaned.split(" ")
eng2_tokenized = eng2_cleaned.split(" ")
cat1_tokenized = cat1_cleaned.split(" ")
cat2_tokenized = cat2_cleaned.split(" ")

# Normalization of the tokens to get more accurately the frequencies, without differences between uppercase and lowercase letters.
eng1_normed = [w.lower() for w in eng1_tokenized]
eng2_normed = [w.lower() for w in eng2_tokenized]
cat1_normed = [w.lower() for w in cat1_tokenized]
cat2_normed = [w.lower() for w in cat2_tokenized]

## COUNTING FREQUENCIES
from collections import Counter # Importing the Counter from the collections library to count the number of occurrences of each word in the books.

# Counting the frequencies and saving them into a variable for each book
eng1_freq = Counter(eng1_normed)
eng2_freq = Counter(eng2_normed)
cat1_freq = Counter(cat1_normed)
cat2_freq = Counter(cat2_normed)

# Function for calculating the mean of the length of the words in a list.
def mean_length(list_words):
    total_letters = sum(len(word) for word, _ in list_words) # Adding the number of letters of each word in the list
    total_words = len(list_words) # Counting the number of words in the list

    mean = total_letters / total_words
    return mean

# Extracting and comparing the 10 most frequent words in the two English books.
eng1_most_freq = eng1_freq.most_common(10)
eng2_most_freq = eng2_freq.most_common(10)

print("Most common words in English poems")
print("Eng1 book:", eng1_most_freq)
print("Eng2 book:", eng2_most_freq)

print("Average length of the words")
print("Eng1 book:", mean_length(eng1_most_freq[1:])) # The index [1:] is included to avoid the first word, because it is empty.
print("Eng2 book:", mean_length(eng2_most_freq[1:]))

# Extracting and comparing the 10 least frequent words in the two English books.
eng1_least_freq = eng1_freq.most_common()[-11:-1] # Index used to obtain the 10 elements at the bottom, which are the least frequent words.
eng2_least_freq = eng2_freq.most_common()[-11:-1]

print("Least common words in English poems")
print("Eng1 book:", eng1_least_freq)
print("Eng2 book:", eng2_least_freq)

print("Average length of the words")
print("Eng1 book:", mean_length(eng1_least_freq))
print("Eng2 book:", mean_length(eng2_least_freq))

# Extracting and comparing the 10 most frequent words in the two Catalan books.
cat1_most_freq = cat1_freq.most_common(10)
cat2_most_freq = cat2_freq.most_common(10)

print("Most common words in Catalan poems")
print("Cat1 book:", cat1_most_freq)
print("Cat2 book:", cat2_most_freq)

print("Average length of the words")
print("Cat1 book:", mean_length(cat1_most_freq[1:])) # The index [1:] is included to avoid the first word, because it is empty.
print("Cat2 book:", mean_length(cat2_most_freq[1:]))

# Extracting and comparing the 10 least frequent words in the two Catalan books.
cat1_least_freq = cat1_freq.most_common()[-11:-1] # Index used to obtain the 10 elements at the bottom, which are the least frequent words.
cat2_least_freq = cat2_freq.most_common()[-11:-1]

print("Least common words in Catalan poems")
print("Cat1 book:", cat1_least_freq)
print("Cat2 book:", cat2_least_freq)

print("Average length of the words")
print("Cat1 book:", mean_length(cat1_least_freq))
print("Cat2 book:", mean_length(cat2_least_freq))